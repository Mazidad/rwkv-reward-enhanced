
# Reward Model based on RWKV (Enhanced Version)

Welcome to the improved version of the RWKV based reward model. This work is in progress and is not ready for use yet.

## Overview

This project brings to you an implementation of a basic reward model using RWKV models. This model training serves two primary objectives:

- To demonstrate the potential of RWKV models in learning reward functions.
- Once the reward model is trained, it can be utilized to train a policy employing RL algorithms. This encourages the underlying RWKV model to generate more diverse trajectories as well as superior answers.

## Running the Experiment

### Pre-requisites

You need to have RWKV official pypi package installed. Install the package using: `pip install rwkv`. Other required packages are `datasets` and `pytorch`.

```
pip install rwkv
pip install datasets
pip install torch
```

### Model Weights

For experiment, a smaller RWKV model with 430M parameters is used for a quicker training and testing. You can download the model weights from here: https://huggingface.co/BlinkDL/rwkv-4-pile-430m

Specifically, we use these weights: https://huggingface.co/BlinkDL/rwkv-4-pile-430m/resolve/main/RWKV-4-Pile-430M-20220808-8066.pth

### Commands

To run the experiment, use the command below:

```
python train.py
```

The reward dataset used in this experiment is from https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets. We sample 100 data points from the training set and 20 from the validation set to demonstrate capabilities of the reward model in predicting reward values.

## More About the Reward Model

The reward model in this setting first reads the prompt and output from another LLM model, then rates the output between 1 to -1 (in logits) for 'accept' and 'reject' respectively. Data from the `yitingxie/rlhf-reward-datasets` dataset is used where each prompt has two answers generated by an unknown LLM, with one answer rated 'accept' and the other 'reject' by human raters. The reward model is trained to predict these human ratings.

With a trained reward model, we can train a policy using RL algorithms that encourages the base RWKV model to get higher scores (more 'accept' ratings).

## Future Work

1. The reward model needs further training. Currently, only the last few layers are trained.
2. We plan to implement qLora on top of this reward model to make it fully trainable.
3. The current dataset used for illustration is not sufficiently large or diverse enough. More data will be collected to train a better reward model.